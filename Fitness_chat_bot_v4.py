from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from flask import Flask, request, jsonify

# Load environment variables
load_dotenv()

# Initialize Flask app
app = Flask(__name__)

# Initialize the language model (without tools)
llm = ChatGroq(
    model="mixtral-8x7b-32768",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
)

# Set up the system message (updated for better formatting and context awareness)
system_message = SystemMessage(content="""You are a fitness chatbot. You can engage in general fitness-related conversations, remembering previous interactions to provide context. 
When asked about BMI, calculate it accurately using the formula: BMI = weight (kg) / (height (m)²), where height is in meters (convert cm to m by dividing by 100), and format the response as:  
**BMI Result:** [value] kg/m²  
This is considered [category, e.g., 'normal', 'overweight'].  
Provide suggestions if the BMI indicates health concerns.  
When asked about the calories and protein in a meal, provide estimated values in a formatted way:  
**Calories:** [estimate] kcal  
**Protein:** [estimate] g  
Include additional nutritional tips if relevant.  
If the meal description is unclear, ask for more details.  
Use concise, accurate, friendly responses with markdown-like formatting (e.g., **bold**, *italics*, line breaks) for readability.  
Maintain context from previous messages to answer follow-ups effectively.""")

# Initialize conversation history (persisted in memory for this example)
conversation_history = [system_message]

# Define the chatbot response function
def chatbot_response(user_input):
    conversation_history.append(HumanMessage(content=user_input))
    
    response = llm.invoke(conversation_history)
    
    if hasattr(response, 'content') and response.content:
        formatted_response = response.content.strip()  # Clean up response
        conversation_history.append(AIMessage(content=formatted_response))
        return formatted_response
    else:
        return "Error: No response generated by the model."

# Flask API endpoint (now includes conversation history in response for UI)
@app.route('/chat', methods=['POST'])
def chat():
    data = request.get_json()
    user_input = data.get('message', '')
    if not user_input:
        return jsonify({'error': 'No message provided'}), 400
    try:
        response = chatbot_response(user_input)
        # Return both the response and the full conversation history for UI display
        return jsonify({
            'response': response,
            'history': [
                {'role': 'user', 'content': msg.content} if isinstance(msg, HumanMessage) 
                else {'role': 'assistant', 'content': msg.content} 
                for msg in conversation_history[1:]  # Skip system message
            ]
        })
    except Exception as e:
        return jsonify({'error': f'Internal server error: {str(e)}'}), 500

# Serve the HTML file
@app.route('/')
def index():
    return app.send_static_file('index.html')

if __name__ == "__main__":
    app.run(debug=True)